# LLM Training (Maxim Panchuk)
1. Open ipynb in Google Colab Pro with A100 GPU
2. Place arXiv.bin - pretokenized dataset at the same level of llama.ipynb
3. Start training
4. Every 1000 iterations ckpt is saving if eval loss is better than previous
